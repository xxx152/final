
## 問題設計
最終目標是 AI 伺服器能預測用戶行為、平行模擬多條路徑，讓線上互動感覺零延遲。這個簡化問題對應如下：

預測行為：使用 LSTM 從歷史狀態預測玩家下一步動作（top-2 可能性），對應未來 AI 預測複雜用戶輸入（如遊戲指令或 UI 操作）。
模擬延遲與補償：模擬 RTT 延遲，AI 事先發送預測，讓客戶端應用動作，對應最終的「事先平行計算路徑」，減少感知延遲。
環境互動：格子世界代表簡化遊戲/互動場景，收集金幣任務模擬用戶目標（如遊戲勝利或應用完成），AI 學習最佳策略對應個性化預測。
逐步邁進：成功驗證單玩家預測後，可擴展到多玩家、真實網路和複雜狀態（如 3D 或多模態輸入），橋接 20 年後的全面應用。


### 輸入：
遊戲狀態：格子世界參數（寬 16、高 12），玩家位置 (ax, ay)、障礙物和金幣 (NUM_REWARDS=5) 位置。
歷史數據：過去動作序列 (ACTION_HISTORY=6)，編碼為 one-hot 向量。
資料形式：NumPy 陣列，狀態特徵為浮點數向量（尺寸 INPUT_DIM = 14 基本特徵 + 30 動作歷史 = 44），包含正規化位置、最近 3 金幣/障礙向量。

### 輸出：
RL 階段：Q 值 (NUM_ACTIONS=4)，用於選擇最佳動作。
LSTM 階段：動作概率分佈 (softmax 輸出)，用於預測 top-2 動作。
模擬遊戲：預測準確率 (acc%) 和收集分數 (score)。

### 任務目標：
訓練 RL 代理最大化收集金幣分數 (reward: +10 金幣, -1 碰撞, -0.1 移動)。
訓練 LSTM 預測 RL 專家的動作，達成 >80% top-2 準確率。
在模擬延遲遊戲中，維持高準確率，減少狀態不一致 (回滾)。

### 資料形式：
訓練數據：序列狀態 (SEQ_LEN=8) 和標記動作 (整數 0-3)。
評估：文本輸出如 "Final Accuracy: 85.0%, Final Score: 15"。


## 模型與方法

選擇模型：組合 Q-Network (簡單前饋神經網路 for RL) 和 LSTM (循環神經網路 for 序列預測)。  
Q-Network：三層全連接網路 (nn.Linear: 14→128→128→4)，用於 Q-learning 估計動作價值。  
LSTM：nn.LSTM (輸入 44, 隱藏 128) + nn.Linear 輸出 4 動作 logit，用於預測序列。  
理由：Q-Network 適合 RL 任務，學習環境策略作為「專家」；LSTM 處理時間序列狀態，捕捉歷史依賴，適合行為預測。非線性迴歸或 generative model 不適合，因為需決策優化 (RL) 和序列建模 (LSTM)。  
